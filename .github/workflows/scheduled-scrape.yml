name: Scheduled Scrape

on:
  schedule:
    # Run every 6 hours at minute 0
    - cron: '0 */6 * * *'
  workflow_dispatch:
    # Allow manual triggering from GitHub UI
    inputs:
      reason:
        description: 'Reason for manual trigger'
        required: false
        default: 'Manual content refresh'

env:
  # Default Railway URL - can be overridden with repository secret
  API_URL: ${{ secrets.RAILWAY_API_URL || 'https://web-production-bcd96.up.railway.app' }}
  # API key for authenticated endpoints (required for /api/scrape)
  API_KEY: ${{ secrets.API_KEY }}

jobs:
  scrape:
    name: Trigger Content Scrape
    runs-on: ubuntu-latest

    steps:
      - name: Display trigger info
        run: |
          echo "Scrape triggered at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "Trigger: ${{ github.event_name }}"
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "Reason: ${{ github.event.inputs.reason }}"
          fi

      - name: Check API health
        id: health
        run: |
          echo "Checking API health..."
          HEALTH_RESPONSE=$(curl -s -w "\n%{http_code}" "${{ env.API_URL }}/api/health")
          HTTP_CODE=$(echo "$HEALTH_RESPONSE" | tail -n1)
          BODY=$(echo "$HEALTH_RESPONSE" | sed '$d')

          echo "Health response: $BODY"
          echo "HTTP status: $HTTP_CODE"

          if [ "$HTTP_CODE" != "200" ]; then
            echo "::error::API health check failed with status $HTTP_CODE"
            exit 1
          fi

          echo "API is healthy"

      - name: Run scrape (synchronous)
        id: scrape
        run: |
          echo "Running synchronous scrape..."

          # Check if API key is configured
          if [ -z "${{ env.API_KEY }}" ]; then
            echo "::error::API_KEY secret is not configured. Please add it to repository secrets."
            exit 1
          fi

          # Call sync endpoint â€” waits for pipeline to complete (up to 10 min)
          HTTP_RESPONSE=$(curl -s -w "\n%{http_code}" \
            --max-time 600 \
            -X POST "${{ env.API_URL }}/api/scrape/sync" \
            -H "Content-Type: application/json" \
            -H "X-API-Key: ${{ env.API_KEY }}")

          HTTP_CODE=$(echo "$HTTP_RESPONSE" | tail -n1)
          BODY=$(echo "$HTTP_RESPONSE" | sed '$d')

          echo "HTTP status: $HTTP_CODE"
          echo "Response: $BODY"

          # Extract fields using grep/sed (no jq dependency)
          JOB_ID=$(echo "$BODY" | grep -o '"job_id":"[^"]*"' | sed 's/"job_id":"//;s/"//')
          STATUS=$(echo "$BODY" | grep -o '"status":"[^"]*"' | sed 's/"status":"//;s/"//')
          RESULT_COUNT=$(echo "$BODY" | grep -o '"result_count":[0-9]*' | sed 's/"result_count"://')
          ERROR_MSG=$(echo "$BODY" | grep -o '"error_message":"[^"]*"' | sed 's/"error_message":"//;s/"//')

          echo "job_id=$JOB_ID" >> $GITHUB_OUTPUT
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "result_count=$RESULT_COUNT" >> $GITHUB_OUTPUT

          if [ "$HTTP_CODE" != "200" ]; then
            echo "::error::Scrape request failed with HTTP $HTTP_CODE"
            if [ -n "$ERROR_MSG" ]; then
              echo "::error::Error: $ERROR_MSG"
            fi
            exit 1
          fi

          if [ "$STATUS" = "completed" ]; then
            echo "Scrape completed successfully!"
            echo "Articles stored: $RESULT_COUNT"
          elif [ "$STATUS" = "failed" ]; then
            echo "::error::Scrape job failed: $ERROR_MSG"
            exit 1
          else
            echo "::warning::Unexpected job status: $STATUS"
          fi

      - name: Report results
        if: always()
        run: |
          echo "============================================"
          echo "SCRAPE JOB SUMMARY"
          echo "============================================"
          echo "Timestamp: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "Job ID: ${{ steps.scrape.outputs.job_id || 'N/A' }}"
          echo "Status: ${{ steps.scrape.outputs.status || 'unknown' }}"
          echo "Articles stored: ${{ steps.scrape.outputs.result_count || 'N/A' }}"
          echo "============================================"
