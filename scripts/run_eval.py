"""
Evaluation harness: compare classification methods on the eval set.

Computes per-category precision/recall/F1 and macro-F1.
Compares LLM vs keyword classifications, and vs human labels if available.

Usage:
    python scripts/run_eval.py

Input: data/eval_set.csv (generated by create_eval_set.py)
"""

import csv
import os
import sys
from collections import defaultdict

EVAL_CSV = os.path.join(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
    "data",
    "eval_set.csv",
)

CATEGORIES = ["startups", "policy", "research", "industry"]


def load_eval_set(path: str) -> list[dict]:
    """Load evaluation set from CSV."""
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    return rows


def compute_metrics(
    predictions: list[str], ground_truth: list[str], categories: list[str]
) -> dict:
    """Compute per-category precision/recall/F1 and macro-F1."""
    metrics = {}

    for cat in categories:
        tp = sum(1 for p, g in zip(predictions, ground_truth) if p == cat and g == cat)
        fp = sum(1 for p, g in zip(predictions, ground_truth) if p == cat and g != cat)
        fn = sum(1 for p, g in zip(predictions, ground_truth) if p != cat and g == cat)

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = (
            2 * precision * recall / (precision + recall)
            if (precision + recall) > 0
            else 0.0
        )

        metrics[cat] = {
            "precision": round(precision, 3),
            "recall": round(recall, 3),
            "f1": round(f1, 3),
            "support": sum(1 for g in ground_truth if g == cat),
        }

    # Macro-F1 (only over categories with support > 0)
    active = [m for m in metrics.values() if m["support"] > 0]
    macro_f1 = sum(m["f1"] for m in active) / len(active) if active else 0.0
    metrics["macro_f1"] = round(macro_f1, 3)

    return metrics


def print_confusion_matrix(
    predictions: list[str], ground_truth: list[str], categories: list[str]
) -> None:
    """Print a confusion matrix."""
    matrix = defaultdict(lambda: defaultdict(int))
    for p, g in zip(predictions, ground_truth):
        matrix[g][p] += 1

    # Header
    header = f"{'True \\ Pred':<15}" + "".join(f"{c:<12}" for c in categories)
    print(header)
    print("-" * len(header))

    for true_cat in categories:
        row = f"{true_cat:<15}"
        for pred_cat in categories:
            row += f"{matrix[true_cat][pred_cat]:<12}"
        print(row)


def print_metrics(metrics: dict, label: str) -> None:
    """Print formatted metrics table."""
    print(f"\n{'Category':<15} {'Prec':>8} {'Recall':>8} {'F1':>8} {'Support':>8}")
    print("-" * 50)
    for cat in CATEGORIES:
        if cat in metrics:
            m = metrics[cat]
            print(
                f"{cat:<15} {m['precision']:>8.3f} {m['recall']:>8.3f} "
                f"{m['f1']:>8.3f} {m['support']:>8}"
            )
    print("-" * 50)
    print(f"{'Macro F1':<15} {'':>8} {'':>8} {metrics['macro_f1']:>8.3f}")


def main():
    if not os.path.exists(EVAL_CSV):
        print(f"Eval set not found: {EVAL_CSV}")
        print("Run: python scripts/create_eval_set.py first")
        sys.exit(1)

    rows = load_eval_set(EVAL_CSV)
    print(f"Loaded {len(rows)} evaluation articles\n")

    kw_preds = [r["keyword_category"] for r in rows]
    llm_preds = [r["llm_category"] for r in rows]
    human_labels = [r.get("human_category", "").strip() for r in rows]

    has_human = any(human_labels)

    # --- LLM vs Keyword agreement ---
    agree = sum(1 for k, l in zip(kw_preds, llm_preds) if k == l)
    print("=== LLM vs Keyword Agreement ===")
    print(f"Agreement: {agree}/{len(rows)} ({agree * 100 // len(rows)}%)")
    print("\nConfusion (rows=keyword, cols=LLM):")
    print_confusion_matrix(llm_preds, kw_preds, CATEGORIES)

    # Disagreements
    disagree = [
        (r["title"][:50], r["keyword_category"], r["llm_category"])
        for r in rows
        if r["keyword_category"] != r["llm_category"]
    ]
    if disagree:
        print(f"\nDisagreements ({len(disagree)}):")
        for title, kw, llm in disagree[:15]:
            print(f"  {title}... | kw={kw} llm={llm}")
        if len(disagree) > 15:
            print(f"  ... and {len(disagree) - 15} more")

    # --- Metrics vs human labels (if available) ---
    if has_human:
        # Filter to rows with human labels
        labeled = [(r, h) for r, h in zip(rows, human_labels) if h]
        print(f"\n\n=== Metrics vs Human Labels ({len(labeled)} labeled) ===")

        h_labels = [h for _, h in labeled]
        h_kw = [r["keyword_category"] for r, _ in labeled]
        h_llm = [r["llm_category"] for r, _ in labeled]

        print("\n--- Keyword Classifier ---")
        kw_metrics = compute_metrics(h_kw, h_labels, CATEGORIES)
        print_metrics(kw_metrics, "Keyword")

        print("\n--- LLM Classifier ---")
        llm_metrics = compute_metrics(h_llm, h_labels, CATEGORIES)
        print_metrics(llm_metrics, "LLM")

        print(f"\nKeyword macro-F1: {kw_metrics['macro_f1']}")
        print(f"LLM macro-F1:     {llm_metrics['macro_f1']}")
        target = 0.85
        status = "PASS" if llm_metrics["macro_f1"] >= target else "FAIL"
        print(f"Target (>= {target}): {status}")
    else:
        print("\n\nNo human labels found in eval set.")
        print("To compute accuracy metrics, fill in the 'human_category' column")
        print(f"in {EVAL_CSV} and re-run this script.")

    # --- Category distribution comparison ---
    print("\n\n=== Category Distribution ===")
    print(f"{'Category':<15} {'Keyword':>10} {'LLM':>10}")
    print("-" * 35)
    for cat in CATEGORIES:
        kw_count = kw_preds.count(cat)
        llm_count = llm_preds.count(cat)
        print(f"{cat:<15} {kw_count:>10} {llm_count:>10}")


if __name__ == "__main__":
    main()
